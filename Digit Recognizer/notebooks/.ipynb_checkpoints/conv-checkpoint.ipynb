{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000,)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_X = train.iloc[:,1:]\n",
    "train_y = train['label']\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test.shape)\n",
    "\n",
    "del train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_X = train_X / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "train_X = train_X.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "train_y = to_categorical(train_y, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9b4c6c5ba8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (2,2),\n",
    "                 padding = 'Same', \n",
    "                 activation ='relu', \n",
    "                 input_shape = (28,28,1)))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "opt = Adam(lr=0.012, decay=0.001) # the decay param decreases the learning rate of that amount at each epoch\n",
    "\n",
    "model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "525/525 [==============================] - 24s 46ms/step - loss: 0.4158 - categorical_accuracy: 0.8660 - val_loss: 0.1232 - val_categorical_accuracy: 0.9626\n",
      "Epoch 2/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.2110 - categorical_accuracy: 0.9329 - val_loss: 0.0820 - val_categorical_accuracy: 0.9757\n",
      "Epoch 3/15\n",
      "525/525 [==============================] - 24s 46ms/step - loss: 0.1798 - categorical_accuracy: 0.9450 - val_loss: 0.0908 - val_categorical_accuracy: 0.9762\n",
      "Epoch 4/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1555 - categorical_accuracy: 0.9523 - val_loss: 0.0793 - val_categorical_accuracy: 0.9785\n",
      "Epoch 5/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1431 - categorical_accuracy: 0.9557 - val_loss: 0.0749 - val_categorical_accuracy: 0.9796\n",
      "Epoch 6/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1292 - categorical_accuracy: 0.9585 - val_loss: 0.0815 - val_categorical_accuracy: 0.9760\n",
      "Epoch 7/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1206 - categorical_accuracy: 0.9628 - val_loss: 0.0752 - val_categorical_accuracy: 0.9776\n",
      "Epoch 8/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1128 - categorical_accuracy: 0.9649 - val_loss: 0.0583 - val_categorical_accuracy: 0.9833\n",
      "Epoch 9/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1110 - categorical_accuracy: 0.9659 - val_loss: 0.0633 - val_categorical_accuracy: 0.9827\n",
      "Epoch 10/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.1067 - categorical_accuracy: 0.9685 - val_loss: 0.0682 - val_categorical_accuracy: 0.9810\n",
      "Epoch 11/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.0986 - categorical_accuracy: 0.9698 - val_loss: 0.0533 - val_categorical_accuracy: 0.9851\n",
      "Epoch 12/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.0908 - categorical_accuracy: 0.9718 - val_loss: 0.0498 - val_categorical_accuracy: 0.9857\n",
      "Epoch 13/15\n",
      "525/525 [==============================] - 24s 45ms/step - loss: 0.0887 - categorical_accuracy: 0.9721 - val_loss: 0.0551 - val_categorical_accuracy: 0.9845\n",
      "Epoch 14/15\n",
      "525/525 [==============================] - 24s 46ms/step - loss: 0.0879 - categorical_accuracy: 0.9723 - val_loss: 0.0480 - val_categorical_accuracy: 0.9858\n",
      "Epoch 15/15\n",
      "525/525 [==============================] - 24s 46ms/step - loss: 0.0838 - categorical_accuracy: 0.9737 - val_loss: 0.0534 - val_categorical_accuracy: 0.9869\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "datagen.fit(train_X)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(train_X, \n",
    "                                 train_y, \n",
    "                                 batch_size=64),\n",
    "                    validation_data = (val_X, val_y),\n",
    "                    epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(val_X)\n",
    "print('RMSE = %.6f' % math.sqrt(mean_squared_error(pred_y, val_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(len(history.history['val_categorical_accuracy'])), y=history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "sns.lineplot(x=range(len(history.history['categorical_accuracy'])), y=history.history['categorical_accuracy'], label='categorical_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(val_y.argmax(axis=1), pred_y.argmax(axis=1))\n",
    "\n",
    "sns.heatmap(matrix, cbar=False, annot=True, fmt='d', cmap=\"Blues\", square=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = model.predict(test)\n",
    "test_y = test_y.round()\n",
    "out_label = [ np.argmax(i) for i in test_y]\n",
    "out_imageid = [ i+1 for i in range(len(test_y))]\n",
    "out = pd.DataFrame()\n",
    "out['ImageId'] = out_imageid\n",
    "out['Label'] = out_label\n",
    "\n",
    "out.to_csv('submission.csv', index=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
